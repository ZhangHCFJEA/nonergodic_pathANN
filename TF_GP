#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Wed Jul  1 15:45:54 2020

@author: aklimasewski
"""


import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import pandas as pd

from sklearn.preprocessing import PowerTransformer


import tensorflow.compat.v2 as tf
tf.enable_v2_behavior()

import tensorflow_probability as tfp

sns.reset_defaults()
#sns.set_style('whitegrid')
#sns.set_context('talk')
sns.set_context(context='talk',font_scale=0.7)


tfd = tfp.distributions


# negloglik = lambda y, rv_y: -rv_y.log_prob(y)



###############
#recreate the demo with one input and one output only

# %%

def transform_dip(diptrain,diptest,rxtrain,rxtest):
    for i in range(len(diptrain)):
        if diptrain[i]>30:
            rxtrain[i]=rxtrain[i]*(90-diptrain[i])/45
        else:
            rxtrain[i]=rxtrain[i]*60/45
            
    for i in range(len(diptest)): 
        if diptest[i]>30:
            rxtest[i]=rxtest[i]*(90-diptest[i])/45
        else:
            rxtest[i]=rxtest[i]*60/45    
    #return the transformed arrays
    return diptrain, diptest, rxtrain, rxtest


def readindata(nametrain, nametest):
#Read in datasets
# nametrain='/Users/aklimasewski/Documents/data/cybertrainyeti10_residfeb.csv'
# nametest='/Users/aklimasewski/Documents/data/cybertestyeti10_residfeb.csv'
    
    dftrain = pd.read_pickle(nametrain) 
    dftest = pd.read_pickle(nametest)
    print(dftrain.shape)
    print(dftest.shape)
    
    
    #separate dataframe to pandas series (1 col of df)
    Mwtrain= dftrain["Mag"]
    distrain=dftrain["Site_Rupture_Dist"]
    vs30train=np.array(dftrain["vs30"])
    z10train=dftrain["z10"]
    z25train=dftrain["z25"]
    lattrain=dftrain["CS_Site_Lat"]
    longtrain=dftrain["CS_Site_Lon"]
    periodtrain=dftrain["siteperiod"]
    hypolattrain=dftrain["Hypocenter_Lat"]
    hypolontrain=dftrain["Hypocenter_Lon"]
    hypodepthtrain=dftrain["Hypocenter_Depth"]
    raketrain=dftrain["Rake_y"]
    diptrain=dftrain["Dip_y"]
    striketrain=dftrain["Strike_y"]+180
    widthtrain=dftrain["Width"]
    #Outputs (col per period)
    residtesttemp=dftrain.loc[:, 'IM_Value':'IM175']
    train_targets1=residtesttemp.values
    
    #histogram of output (1 period of IM175)
    plt.hist(train_targets1[:,3],100)
    
    lengthtrain=dftrain["Length"]
    rjbtrain=dftrain["rjb"]
    rxtrain=dftrain["rx"]
    rytrain=dftrain["ry"]
    hypodistrain=dftrain["hypodistance"]
    Utrain=dftrain["U"]
    Ttrain=dftrain["T"]
    xitrain=dftrain["xi"]
    startdepthtrain=dftrain["Start_Depth"]
    
    
    #### same with testdata
    Mwtest= dftest["Mag"]
    distest=dftest["Site_Rupture_Dist"]
    vs30test=np.array(dftest["vs30"])
    z10test=dftest["z10"]
    z25test=dftest["z25"]
    lattest=dftest["CS_Site_Lat"]
    longtest=dftest["CS_Site_Lon"]
    periodtest=dftest["siteperiod"]
    hypolattest=dftest["Hypocenter_Lat"]
    hypolontest=dftest["Hypocenter_Lon"]
    hypodepthtest=dftest["Hypocenter_Depth"]
    raketest=dftest["Rake_y"]
    diptest=dftest["Dip_y"]
    striketest=dftest["Strike_y"]+180
    widthtest=dftest["Width"]
    residtesttemp1=dftest.loc[:, 'IM_Value':'IM175']
    test_targets1=residtesttemp1.values
    lengthtest=dftest["Length"]
    rjbtest=dftest["rjb"]
    rxtest=dftest["rx"]
    rytest=dftest["ry"]
    hypodistest=dftest["hypodistance"]
    Utest=dftest["U"]
    Ttest=dftest["T"]
    xitest=dftest["xi"]
    startdepthtest=dftest["Start_Depth"]
    
    diptrain, diptest, rxtrain, rxtest = transform_dip(diptrain=np.array(diptrain),diptest=np.array(diptest),rxtrain=np.array(rxtrain),rxtest=np.array(rxtest))
    
    #put together arrays of features for ANN
    train_data1 = np.column_stack([Mwtrain,distrain,vs30train,z10train,z25train,raketrain,diptrain,hypodepthtrain, widthtrain,
                                   rjbtrain,rxtrain,startdepthtrain])
    test_data1 = np.column_stack([Mwtest,distest,vs30test,z10test,z25test,raketest,diptest, hypodepthtest, widthtest,
                                 rjbtest,rxtest,startdepthtest])
    
    feature_names=['Mw','Rrup','Vs30', 'Z1.0', 'Z2.5', 'Rake','Dip','Hypo_depth', 'Width',
                   'Rjb','Rx','Ztor',]
    return train_data1, test_data1, train_targets1, test_targets1, feature_names



train_data1, test_data1, train_targets1, test_targets1, feature_names = readindata(nametrain='/Users/aklimasewski/Documents/data/cybertrainyeti10_residfeb.csv', nametest='/Users/aklimasewski/Documents/data/cybertestyeti10_residfeb.csv')
#Read in datasets
# nametrain='/Users/aklimasewski/Documents/data/cybertrainyeti10_residfeb.csv'
# nametest='/Users/aklimasewski/Documents/data/cybertestyeti10_residfeb.csv'
#%%
#########################

#preprocessing transform inputs data to be guassian shaped
pt = PowerTransformer()
aa=pt.fit(train_data1[:,:])
train_data=aa.transform(train_data1)
test_data=aa.transform(test_data1)

train_targets = train_targets1
test_targets = test_targets1

y_test = test_targets1.T[0]

y = train_targets1.T[0]

x_range = [[min(train_data.T[i]) for i in range(len(train_data[0]))],[max(train_data.T[i]) for i in range(len(train_data[0]))]]


class RBFKernelFn(tf.keras.layers.Layer):
  def __init__(self, **kwargs):
    super(RBFKernelFn, self).__init__(**kwargs)
    dtype = kwargs.get('dtype', None)

    self._amplitude = self.add_variable(
            initializer=tf.constant_initializer(0),
            dtype=dtype,
            name='amplitude')
    
    self._length_scale = self.add_variable(
            initializer=tf.constant_initializer(0),
            dtype=dtype,
            name='length_scale')

  def call(self, x):
    # Never called -- this is just a layer so it can hold variables
    # in a way Keras understands.
    return x

  @property
  def kernel(self):
     return tfp.math.psd_kernels.MaternOneHalf(
       amplitude=tf.nn.softplus(2.5 * self._amplitude),
       length_scale=tf.nn.softplus(10. * self._length_scale)
     )



# For numeric stability, set the default floating-point dtype to float64
tf.keras.backend.set_floatx('float32')

# Build model.
# points to sample your data range
num_inducing_points = 40

model = tf.keras.Sequential([
    tf.keras.layers.InputLayer(input_shape=(train_data.shape[1],),dtype='float32'),
    tf.keras.layers.Dense(12, kernel_initializer='zeros', use_bias=False),
    tfp.layers.VariationalGaussianProcess(
        num_inducing_points=num_inducing_points,
        kernel_provider=RBFKernelFn(),
        event_shape=[1],
        # inducing_index_points_initializer=tf.constant_initializer(np.linspace((min(train_data.T[0]),min(train_data.T[1]),min(train_data.T[2])),(max(train_data.T[0]),max(train_data.T[1]),max(train_data.T[2])),num_inducing_points,dtype='float32')),
        inducing_index_points_initializer=tf.constant_initializer(np.linspace(*x_range,num_inducing_points,dtype='float32')),
        unconstrained_observation_noise_variance_initializer=(
            tf.constant_initializer(0.1))
            # tf.constant_initializer(0.1)),
    ),
])


batch_size=264

# batch_size = 64
loss = lambda y, rv_y: rv_y.variational_loss(
    y, kl_weight=np.array(batch_size) / train_data.shape[0])

model.compile(optimizer=tf.optimizers.Adam(learning_rate=0.001), loss=loss,metrics=['mae','mse'])
# model.fit(train_data, y, batch_size=batch_size, epochs=10, verbose=True)



history=model.fit(train_data,y,validation_data=(test_data,y_test),epochs=10,batch_size=264,verbose=1)
mae_history=history.history['val_mae']
mae_history_train=history.history['mae']
test_mse_score,test_mae_score,tempp=model.evaluate(test_data,y_test)


yhat = model.predict(test_data)

yhat = np.ndarray.flatten(yhat)








#######



# #print simple test
# #10 examples
# stest = np.asarray([test_data1[1175] for i in range(10)])
# #vary distances for plotting
# dist_test = [1,5,10,20,40,50,75,100,120,150]
# for i in range(len(stest)):
#     stest[i][1] = dist_test[i]


# yhat = model(stest)

# plt.figure(figsize=[6, 1.5])  # inches
# # plt.plot(x, y, 'b.', label='observed');
# #sample 5 different models
# num_samples = 5
# for i in range(num_samples):
#   sample_ = yhat.sample().numpy()
#   xaxis = dist_test
#   plt.plot(xaxis,sample_[..., 0].T,'r',linewidth=0.9,label='ensemble means' if i == 0 else None);
# plt.xticks(np.linspace(0,160, num=8));
# plt.show()

plt.figure()
plt.scatter(test_data1.T[1],y_test,label='Testing Data', s = 1)
plt.scatter(test_data1.T[1],yhat,label='Model predictions',  s = 1)
plt.xlabel('Distance (km)')
plt.ylabel('Prediction')
plt.title('predictions vs distance (km)')
plt.grid()
plt.show()



f10=plt.figure('Overfitting Test')
plt.plot(mae_history,label='Testing Data')
plt.plot(mae_history_train,label='Training Data')
plt.xlabel('Epoch')
plt.ylabel('Mean Absolute Error')
plt.title('Overfitting Test')
plt.legend()
print(test_mae_score)
plt.grid()
plt.show()




a=model.predict(test_data)
b=model.predict(train_data)
f1=plt.figure('Earthquake Magnitude normalized Prediction')
plt.plot(train_data[:,0],b,'.r', label='train data')
plt.plot(test_data[:,0],a,'.b',label='test data')
plt.xlabel('input')
plt.ylabel('Prediction')
plt.title('Earthquake Magnitude normalized Prediction')
plt.legend()
plt.show()


f11=plt.figure('Joyner-Boore Dist. (km) normalized Prediction')
plt.plot(train_data[:,1],b,'.r',label='train data')
plt.plot(test_data[:,1],a,'.b',label='test data')
plt.xlabel('input')
plt.ylabel('Prediction')
plt.ylabel('Prediction')
plt.title('Joyner-Boore Dist. (km) normalized Prediction')
plt.legend()
plt.show()





