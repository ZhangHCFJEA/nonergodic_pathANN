#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Aug 30 14:36:49 2020

@author: aklimasewski
"""


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Thu May 30 12:30:48 2019

@author: kwithers
"""
from numpy.random import seed
seed(1)
from tensorflow import set_random_seed
set_random_seed(2)

from numpy.random import seed
seed(1)
from tensorflow import set_random_seed
set_random_seed(2)
import numpy as np

import matplotlib.pyplot as plt
from scipy.fftpack import fft
import obspy
from obspy.core import Trace,Stream,UTCDateTime
from numpy.linalg import inv, qr
import pandas as pd
from pandas import Series, DataFrame
import time
from itertools import compress
from sklearn.preprocessing import power_transform
from sklearn.preprocessing import PowerTransformer
from keras import models
from keras import layers
from keras import optimizers
from keras.layers import Dropout
import tensorflow as tf
import re
import pickle
#from openquake.hazardlib.gsim.boore_2014 import BooreEtAl2014
import pygmm
import glob
from keras.callbacks import EarlyStopping
import gc
import seaborn as sns; sns.set(style="ticks", color_codes=True)


#from sys import platform as sys_pf
#if sys_pf == 'darwin':
#    import matplotlib
#    matplotlib.use("TkAgg")


#gc.collect()  #deletes ram?
plt.close('all')


    
    
train_data=ztest1
test_data=ztest2

train_targets=residtest1
test_targets=residtest2
    
    
  

nametrain='/Users/kwithers/march/cybertrainyeti10_residfeb.csv'
nametest='/Users/kwithers/march/cybertestyeti10_residfeb.csv'


dftrain = pd.read_pickle(nametrain) 
dftest = pd.read_pickle(nametest)


df1 = pd.read_csv('/Users/kwithers/march/lonlatcyber_TableToExcel3.csv')

print(dftrain.shape)
print(dftest.shape)

dftrain=pd.merge_asof(dftrain.sort_values('CS_Site_Lat'), df1.sort_values('CS_Site_Lat'), on='CS_Site_Lat', direction='nearest')
dftest=pd.merge_asof(dftest.sort_values('CS_Site_Lat'), df1.sort_values('CS_Site_Lat'), on='CS_Site_Lat', direction='nearest')


plt.close('all')
Mwtrain= dftrain["Mag"]
distrain=dftrain["Site_Rupture_Dist"]
vs30train=np.array(dftrain["vs30"])


#index=(vs30train<500)
#vs30train[index]=500


z10train=dftrain["z10"]
z25train=dftrain["z25"]
lattrain=dftrain["CS_Site_Lat"]
longtrain=dftrain["CS_Site_Lon"]
periodtrain=dftrain["siteperiod"]

hypolattrain=dftrain["Hypocenter_Lat"]
hypolontrain=dftrain["Hypocenter_Lon"]
hypodepthtrain=dftrain["Hypocenter_Depth"]
raketrain=dftrain["Rake_y"]
diptrain=dftrain["Dip_y"]
striketrain=dftrain["Strike_y"]+180
widthtrain=dftrain["Width"]

residtesttemp=dftrain.loc[:, 'IM_Value':'IM175']
#train_targets1=np.log(residtesttemp.values/100)
train_targets1=residtesttemp.values


plt.close('all')
plt.hist(train_targets1[:,3],100)
#pause

lengthtrain=dftrain["Length"]
rjbtrain=dftrain["rjb"]
rxtrain=dftrain["rx"]
rytrain=dftrain["ry"]
hypodistrain=dftrain["hypodistance"]

Utrain=dftrain["U"]
Ttrain=dftrain["T"]
xitrain=dftrain["xi"]


startdepthtrain=dftrain["Start_Depth"]


b1train=dftrain["dist_z1pt0_50m"]
b2train=dftrain["dist_z1pt0_100m"]
b3train=dftrain["dist_z1pt0_200m"]
b4train=dftrain["dist_z1pt0_300m"]
b5train=dftrain["dist_z1pt0_400m"]
b6train=dftrain["dist_z1pt0_500m"]

########################################################
Mwtest= dftest["Mag"]
distest=dftest["Site_Rupture_Dist"]
vs30test=np.array(dftest["vs30"])

#index=(vs30test<500)
#vs30test[index]=500


z10test=dftest["z10"]
z25test=dftest["z25"]
lattest=dftest["CS_Site_Lat"]
longtest=dftest["CS_Site_Lon"]
periodtest=dftest["siteperiod"]

hypolattest=dftest["Hypocenter_Lat"]
hypolontest=dftest["Hypocenter_Lon"]
hypodepthtest=dftest["Hypocenter_Depth"]
raketest=dftest["Rake_y"]
diptest=dftest["Dip_y"]
striketest=dftest["Strike_y"]+180
widthtest=dftest["Width"]


#Rake: -90:180
#Strike:-180:180
#dip: 0-90

residtesttemp1=dftest.loc[:, 'IM_Value':'IM175']
#test_targets1=np.log(residtesttemp1.values/100)
test_targets1=residtesttemp1.values


lengthtest=dftest["Length"]
rjbtest=dftest["rjb"]
rxtest=dftest["rx"]
rytest=dftest["ry"]
hypodistest=dftest["hypodistance"]

Utest=dftest["U"]
Ttest=dftest["T"]
xitest=dftest["xi"]

startdepthtest=dftest["Start_Depth"]

b1test=dftest["dist_z1pt0_50m"]
b2test=dftest["dist_z1pt0_100m"]
b3test=dftest["dist_z1pt0_200m"]
b4test=dftest["dist_z1pt0_300m"]
b5test=dftest["dist_z1pt0_400m"]
b6test=dftest["dist_z1pt0_500m"]
######################################################## 




diptrain=np.array(diptrain)
diptest=np.array(diptest)
rxtrain=np.array(rxtrain)
rxtest=np.array(rxtest)

rec1=-1
for ij in diptrain:
    rec1=rec1+1
    if diptrain[rec1]>30:
        rxtrain[rec1]=rxtrain[rec1]*(90-diptrain[rec1])/45
    else:
        rxtrain[rec1]=rxtrain[rec1]*60/45

rec1=-1
for ij in diptest:
    rec1=rec1+1    
    if diptest[rec1]>30:
        rxtest[rec1]=rxtest[rec1]*(90-diptest[rec1])/45
    else:
        rxtest[rec1]=rxtest[rec1]*60/45    




train_data1 = np.column_stack([Mwtrain,distrain,vs30train,z10train,z25train,raketrain,diptrain,hypodepthtrain, widthtrain,
                               rjbtrain,rxtrain,startdepthtrain])#,xitrain])#,
                               #xitrain,hypodistrain, b2train, lengthtrain,rytrain,Utrain,Ttrain])
#rytrain,
test_data1 = np.column_stack([Mwtest,distest,vs30test,z10test,z25test,raketest,diptest, hypodepthtest, widthtest,
                             rjbtest,rxtest,startdepthtest])#,xitest])#,
                             #xitest,hypodistest, b2test, lengthtest])#,rytest,Utest,Ttest])



test_datatemp=test_data
    
train_data=np.row_stack([train_data,train_data1])
test_data=np.row_stack([test_data,test_data1])
    

train_targets1=np.delete(train_targets1,[3],1)
test_targets1=np.delete(test_targets1,[3],1)

train_targets1=train_targets1[:,0:6]
test_targets1=test_targets1[:,0:6]

#pause



             
train_targets=np.row_stack([train_targets,train_targets1])
test_targets=np.row_stack([test_targets,test_targets1])

plt.hist(test_targets[:,3],100)
#

feature_names=['Mw','Rrup','Vs30', 'Z1.0', 'Z2.5', 'Rake','Dip','Hypo_depth', 'Width',
               'Rjb','Rx','Ztor','xirpime']
#,'Ry0'

ztestcyber=train_data
ztestcybertest=test_data

dataset = pd.DataFrame({'Mw': train_data[:, 0], 'R_rup': train_data[:, 1],'Vs30': train_data[:, 2],'Rake': train_data[:, 5],'Dataset':'Training'})

#dataset = pd.DataFrame({'Mw': train_data[:, 0], 'R_rup': train_data[:, 1],'Vs30': train_data[:, 2],'Z1.0': train_data[:, 3],'Z2.5': train_data[:, 4],
#    'Rake': train_data[:, 5],'Dip': train_data[:, 6],'Strike': train_data[:, 7],'Hypo_Depth': train_data[:, 8],'Width': train_data[:, 9]})
#    'Length': train_data[:, 10],'R_jb': train_data[:, 11],'Rx': train_data[:, 12],'Ry': train_data[:, 13]})
dataset1 = pd.DataFrame({'Mw': test_data[:, 0], 'R_rup': test_data[:, 1],'Vs30': test_data[:, 2],'Rake': test_data[:, 5],'Dataset':'Testing'})
#f1=plt.figure('Correlations')
#plt.figure(figsize=(16,12))
#corr = dataset.corr()
#sns.heatmap(corr,
#            xticklabels=corr.columns.values,
#            yticklabels=corr.columns.values,cmap='PiYG',center=0)
#pause
#dataset=dataset[1:100]
#pause


dftotal=dataset.append(dataset1) 


f2=plt.figure('Earthquake Magnitude Training Dataset ')
plt.hist(train_data[:,10],100,label='Training Data')
plt.hist(test_data[:,6],100,label='Testing Data')
plt.xlabel('Mw ')
plt.ylabel('Count')
plt.title('Earthquake Magnitude Training/Testing Dataset')
plt.legend()
#pause
#
f2=plt.figure('Earthquake Distance Training Dataset ')
plt.hist(train_data[:,1],100,label='Training Data')
plt.hist(test_data[:,1],100,label='Testing Data')
plt.xlabel('$R_{rup}$ (km)')
plt.ylabel('Count')
plt.title('Closest Distance ($R_{rup}$) Training/Testing Dataset')
plt.legend()

f2=plt.figure('Vs30 ')
plt.hist(train_data[:,2],100,label='Training Data')
plt.hist(test_data[:,2],100,label='Testing Data')
plt.xlabel('$V_{s30}$ (m/s)')
plt.ylabel('Count')
plt.title('$V_{s30}$ (m/s) Training/Testing Dataset')
plt.legend()

f2=plt.figure('Vs301 ')
plt.hist(train_data[:,3],100,label='Training Data')
plt.hist(test_data[:,3],100,label='Testing Data')
plt.xlabel('$V_{s30}$ (m/s)')
plt.ylabel('Count')
plt.title('$V_{s30}$ (m/s) Training/Testing Dataset')
plt.legend()




#period=[10,7.5,5,4,3,2,1,.5,.2,.1]
period=[10,7.5,5,4,3,2,1,0.5,0.2,0.1]
period=[10,7.5,5,3,2,1]
period=np.array(period)


test_data1=test_data

#min-max centered normalization


#################################################################
#f2=plt.figure('Earthquake Magnitude Training Dataset ')
#plt.hist(train_data[:,0],100,label='Training Data')
#plt.hist(test_data[:,0],100,label='Testing Data')
#plt.xlabel('Earthquake Magnitude ')
#plt.ylabel('Count')
#plt.title('Earthquake Magnitude Training/Testing Normalized Dataset')
#plt.legend()
#
#f2=plt.figure('Earthquake Distance Training Dataset ')
#plt.hist(train_data[:,1],100,label='Training Data')
#plt.hist(test_data[:,1],100,label='Testing Data')
#plt.xlabel('Closest Distance: R_rup (km)')
#plt.ylabel('Count')
#plt.title('Closest Distance (R_rup) Training/Testing Normalized Dataset')
#plt.legend()
############################################################################

#################################################################
f2=plt.figure('Targets Training Dataset ')
plt.hist(train_targets[:,5],100,label='Training Targets')
plt.hist(test_targets[:,5],100,label='Testing Targets')
plt.xlabel('SARotD50: log(m/s^2) ')
plt.ylabel('Count')
plt.title('SARotD50 at Period = 0.5 S ')
plt.legend()
#pause

#f2=plt.figure('Earthquake Distance Training Dataset ')
#plt.hist(train_data[:,1],100,label='Training Data')
#plt.hist(test_data[:,1],100,label='Testing Data')
#plt.xlabel('Closest Distance: R_rup (km)')
#plt.ylabel('Count')
#plt.title('Closest Distance (R_rup) Training/Testing Normalized Dataset')
#plt.legend()
############################################################################
#pause
#pt = PowerTransformer()
#aa=pt.fit(train_data[:,:])
#train_data=aa.transform(train_data)
#test_data=aa.transform(test_data)




#pause
pt = PowerTransformer()
aa=pt.fit(train_data[:,:])
train_data=aa.transform(train_data)
test_data=aa.transform(test_data)

#keep1=np.max(train_data,axis=0)
#keep2=np.min(train_data,axis=0)
#keep3=np.mean(train_data,axis=0)
##keep3=(keep1-keep2)/2
#train_data=2./(np.max(train_data,axis=0)-np.min(train_data,axis=0))*(train_data-np.mean(train_data,axis=0))
#test_data=2./(keep1-keep2)*(test_data-keep3)
#origvalues=(train_data/2.*(keep1-keep2))+keep3


#pause
#
#keepmax=np.max(train_data,axis=0)
#keepmax=np.ones(train_data.shape[1])
#train_data=train_data/keepmax
#test_data=aa.transform(test_data)
#test_data=test_data/keepmax
#orig1=train_data*keepmax
#origvalues=aa.inverse_transform(orig1)


f2=plt.figure('Earthquake Magfnitude normalized train ')
plt.hist(train_data[:,0],100)
plt.xlabel('Earthquake Magnitude normalized train')
plt.ylabel('Count')
plt.title('Earthquake Magnitude normalized train Histogram')

#train_data = np.column_stack([Mwtrain,distrain,vs30train,z10train,z25train])
f2=plt.figure(' normalized trainvs30 ')
plt.hist(vs30train,100)

f2=plt.figure(' normalized periodtrain ')
plt.hist(periodtrain,100)

f2=plt.figure(' normalized z10train ')
plt.hist(z10train,100)

f2=plt.figure(' normalized z25train ')
plt.hist(z25train,100)

#plot test data:
f2=plt.figure('Earthquake Magnitude normalized test')
plt.hist(test_data[:,0],100)
plt.xlabel('Earthquake Magnitude normalized test')
plt.ylabel('Count')
plt.title('Earthquake Magnitude normalized test Histogram')

#f3=plt.figure('Joyner-Boore Dist. (km) normalized test')
#plt.hist(test_data[:,1],100)
#plt.xlabel('Joyner-Boore Dist. normalized test')
#plt.ylabel('Count')
#plt.title('Joyner-Boore Dist. normalized test Histogram')

##activation =  ['relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear'] # softmax, softplus, softsign  #logsigmoid





import keras as keras
class Dropout(keras.layers.Dropout):
    """Applies Dropout to the input.
    Dropout consists in randomly setting
    a fraction `rate` of input units to 0 at each update during training time,
    which helps prevent overfitting.
    # Arguments
        rate: float between 0 and 1. Fraction of the input units to drop.
        noise_shape: 1D integer tensor representing the shape of the
            binary dropout mask that will be multiplied with the input.
            For instance, if your inputs have shape
            `(batch_size, timesteps, features)` and
            you want the dropout mask to be the same for all timesteps,
            you can use `noise_shape=(batch_size, 1, features)`.
        seed: A Python integer to use as random seed.
    # References
        - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](
           http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)
    """
    def __init__(self, rate, training=None, noise_shape=None, seed=None, **kwargs):
        super(Dropout, self).__init__(rate, noise_shape=None, seed=None,**kwargs)
        self.training = training

        
    def call(self, inputs, training=None):
        if 0. < self.rate < 1.:
            noise_shape = self._get_noise_shape(inputs)

            def dropped_inputs():
                return K.dropout(inputs, self.rate, noise_shape,
                                 seed=self.seed)
            if not training: 
                return K.in_train_phase(dropped_inputs, inputs, training=self.training)
            return K.in_train_phase(dropped_inputs, inputs, training=training)
        return inputs

from keras.models import Sequential


def build_model():
    #model=models.Sequential()
    model = Sequential()
    #model.add(Dropout(0.0,seed=1))
    model.add(layers.Dense(6,activation='sigmoid', input_shape=(train_data.shape[1],)))
  #  model.add(layers.Dense(3,activation='sigmoid'))
  #  model.add(layers.Dense(7,activation='sigmoid'))
#    model.add(layers.Dense(32,activation='tanh', input_shape=(1,)))
#    model.add(layers.Dense(64, activation='relu'))
    #model.add(Dropout(0.0,seed=1))
    #keras.layers.Dropout(rate, noise_shape=None, seed=None)
    #model.add(layers.Dense(7,activation='sigmoid'))
    
    #model.add(Dropout(0.0,seed=1),training=True)
    model.add(Dropout(rate=0.0, training=True))
    
    model.add(layers.Dense(train_targets.shape[1])) #add sigmoid aciivation functio? (only alues betwen 0 and 1)
    
   # model.compile(optimizer='rmsprop',loss='mse',metrics=['mae']) 
   # model.compile(optimizer=optimizers.RMSprop(lr=1e-4),loss='mse',metrics=['mae']) 
#   
    #model.compile(optimizer=optimizers.RMSprop(lr=4e-2),loss='mse',metrics=['mae','mse']) 
    model.compile(optimizer=optimizers.Adam(lr=2e-3),loss='mse',metrics=['mae','mse']) 
    #model.compile(optimizer='adam',loss='mse',metrics=['mae']) 
    return model

#####################################################################################################################################################################




######################################################################################################################################################
#single fold validation
model=build_model()
print(3)
es = EarlyStopping(monitor='val_mean_absolute_error', mode='max', min_delta=10)
#history=model.fit(train_data,train_targets,validation_data=(test_data,test_targets),epochs=120,batch_size=16,verbose=1,callbacks=[es])

#from numpy.random import seed
#seed(1)
#from tensorflow import set_random_seed
#set_random_seed(2)


history=model.fit(train_data,train_targets,validation_data=(test_data,test_targets),epochs=13,batch_size=256,verbose=1)
mae_history=history.history['val_mae']
mae_history_train=history.history['mae']
test_mse_score,test_mae_score,tempp=model.evaluate(test_data,test_targets)

f10=plt.figure('Overfitting Test')
plt.plot(mae_history,label='Testing Data')
plt.plot(mae_history_train,label='Training Data')
plt.xlabel('Epoch')
plt.ylabel('Mean Absolute Error')
plt.title('Overfitting Test')
plt.legend()
print(test_mae_score)
plt.grid()

#pause


a=model.predict(test_data)
b=model.predict(train_data)
f1=plt.figure('Earthquake Magnitude normalized Prediction')
plt.plot(train_data[:,0],b[:,1],'.r', label='train data')
plt.plot(test_data[:,0],a[:,1],'.b',label='test data')
plt.xlabel('input')
plt.ylabel('Prediction')
plt.title('Earthquake Magnitude normalized Prediction')
plt.show()
plt.legend()

f11=plt.figure('Joyner-Boore Dist. (km) normalized Prediction')
plt.plot(train_data[:,1],b[:,1],'.r',label='train data')
plt.plot(test_data[:,1],a[:,1],'.b',label='test data')
plt.xlabel('input')
plt.ylabel('Prediction')
plt.ylabel('Prediction')
plt.title('Joyner-Boore Dist. (km) normalized Prediction')
plt.show()
plt.legend()



f1=plt.figure('Earthquake Magnitude normalized Actual')
plt.plot(train_data[:,0],train_targets[:,1],'.r')
#plt.plot(test_data[:,0],test_targets[:,0],'.b')
plt.xlabel('input')
plt.ylabel('Prediction')
plt.title('Earthquake Magnitude normalized Actual')
plt.show()

f11=plt.figure('Joyner-Boore Dist. (km) normalized Actual')
plt.plot(train_data[:,1],train_targets[:,1],'.r')
#plt.plot(test_data[:,1],test_targets[:,1],'.b')
plt.xlabel('input')
plt.ylabel('Prediction')
plt.title('Joyner-Boore Dist. (km) normalized Actual')
plt.show()  


f212=plt.figure('T = 5.0 s')
plt.hist(train_targets[:,2]-b[:,2],100,label='Training')
plt.hist(test_targets[:,2]-a[:,2],100,label='Testing')
plt.xlabel('Residual ln(Target/Predicted)')
plt.ylabel('Count')
temp1=str(np.std(train_targets[:,2]-b[:,2]))
temp2=str(np.std(test_targets[:,2]-a[:,2]))
temp11=str(np.mean(train_targets[:,2]-b[:,2]))
temp22=str(np.mean(test_targets[:,2]-a[:,2]))
plt.text(-3.5,10000,   '\u03C3_train = '+ temp1[0:4])
plt.text(-3.5,9000,   '\u03C3_test =' + temp2[0:4])
plt.text(2,10000,   '\u03BC_train =  '+ temp11[0:4])
plt.text(2,9000,   '\u03BC_test = '+ temp22[0:4])
plt.title('Residual ln(Target/Predicted): T = 5.0 s')
plt.legend()
plt.grid()
#pause

f212=plt.figure('T = 0.5 s')
plt.hist(train_targets[:,4]-b[:,4],100,label='Training')
plt.hist(test_targets[:,4]-a[:,4],100,label='Testing')
plt.xlabel('Residual ln(Target/Predicted)')
plt.ylabel('Count')
temp1=str(np.std(train_targets[:,4]-b[:,4]))
temp2=str(np.std(test_targets[:,4]-a[:,4]))
temp11=str(np.mean(train_targets[:,4]-b[:,4]))
temp22=str(np.mean(test_targets[:,4]-a[:,4]))
plt.text(1.2,6000,   'sigma_train = '+ temp1[0:4])
plt.text(1.2,5500,   'sigma_test =' + temp2[0:4])
plt.text(1.2,3000,   'mean_train =  '+ temp11[0:4])
plt.text(1.2,2500,   'mean_test = '+ temp22[0:4])
plt.title('Residual ln(Target/Predicted): T = 0.5 s')
plt.legend()

############ correlation plots
f22=plt.figure('Correlation: T = 5.0 s, Training Data')
plt.plot(train_targets[:,2],b[:,2],'.',label='Training')
plt.xlabel('Targets')
plt.ylabel('Predicted')
plt.title('Correlation: T = 5.0 s, Training Data')
plt.xlim(np.min(train_targets[:,2])-1, np.max(train_targets[:,2])+1)
plt.ylim(np.min(train_targets[:,2])-1, np.max(train_targets[:,2])+1)
plt.plot([np.min(train_targets[:,2]),np.max(train_targets[:,2])],[np.min(train_targets[:,2]),np.max(train_targets[:,2])])
temp=np.corrcoef(train_targets[:,2],b[:,2])
temp=str(temp[0,1])
plt.text(0,-5,   'Training: R ='+ temp[0:4])
plt.grid()

#f22=plt.figure('Correlation: T = 7.5 s, Testing Data')
plt.plot(test_targets[:,2],a[:,2],'.',label='Testing')
#plt.xlabel('Targets')
#plt.ylabel('Predicted')
#plt.title('Correlation: T = 1.0 s, Testing Data')
plt.xlim(np.min(test_targets[:,2])-1, np.max(test_targets[:,2])+1)
plt.ylim(np.min(test_targets[:,2])-1, np.max(test_targets[:,2])+1)
plt.plot([np.min(test_targets[:,2]),np.max(test_targets[:,2])],[np.min(test_targets[:,2]),np.max(test_targets[:,2])])
temp=np.corrcoef(test_targets[:,2],a[:,2])
temp=str(temp[0,1])
plt.text(0,-5.9,   'Testing: R ='+ temp[0:4])
plt.grid()
plt.legend()




#######################3
f22=plt.figure('Correlation: T = 0.5 s, Training Data')
plt.plot(train_targets[:,4],b[:,4],'.',label='Training')
plt.xlabel('Targets')
plt.ylabel('Predicted')
plt.title('Correlation: T = 0.5 s, Training Data')
plt.xlim(np.min(train_targets[:,4])-1, np.max(train_targets[:,4])+1)
plt.ylim(np.min(train_targets[:,4])-1, np.max(train_targets[:,4])+1)
plt.plot([np.min(train_targets[:,4]),np.max(train_targets[:,4])],[np.min(train_targets[:,4]),np.max(train_targets[:,4])])
temp=np.corrcoef(train_targets[:,4],b[:,4])
temp=str(temp[0,1])
plt.text(0,-3,   'Training: R ='+ temp[0:4])
plt.grid()

#f22=plt.figure('Correlation: T = 7.5 s, Testing Data')
plt.plot(test_targets[:,4],a[:,4],'.',label='Testing')
#plt.xlabel('Targets')
#plt.ylabel('Predicted')
#plt.title('Correlation: T = 1.0 s, Testing Data')
plt.xlim(np.min(test_targets[:,4])-1, np.max(test_targets[:,4])+1)
plt.ylim(np.min(test_targets[:,4])-1, np.max(test_targets[:,4])+1)
plt.plot([np.min(test_targets[:,4]),np.max(test_targets[:,4])],[np.min(test_targets[:,4]),np.max(test_targets[:,4])])
temp=np.corrcoef(test_targets[:,4],a[:,4])
temp=str(temp[0,1])
plt.text(0,-4,   'Testing: R ='+ temp[0:4])
plt.grid()
plt.legend()
################################3

diff=np.std(train_targets-b,axis=0)
difftest=np.std(test_targets-a,axis=0)
diffmean=np.mean(train_targets-b,axis=0)
f22=plt.figure('Difference Std of residuals vs Period')
plt.semilogx(period,diff,label='Training ')
plt.semilogx(period,difftest,label='Testing')
plt.xlabel('Period')
plt.ylabel('Total Standard Deviation')

#save
with open('diffb1.txt', 'wb') as filehandle:  
    # store the data as binary data stream
    pickle.dump(diff, filehandle)


plt.xlabel('Period')
plt.ylabel('Total Standard Deviation ')
plt.title('Standard Deviation of Residuals vs Period')




gmpeBSSAdata=np.zeros([period.shape[0]])
gmpeASKdata=np.zeros([period.shape[0]])
gmpeCBdata=np.zeros([period.shape[0]])
gmpeCYdata=np.zeros([period.shape[0]])

gmpeBSSAstd=np.zeros([period.shape[0]])
gmpeASKstd=np.zeros([period.shape[0]])
gmpeCBstd=np.zeros([period.shape[0]])
gmpeCYstd=np.zeros([period.shape[0]])
    ##ztest = np.column_stack([
    #Mwtest: 0
    #distest : 1
    #vs30test: 2
    #z1test : 3
    #z2p5test: 4
    #rake: 5
    #dip : 6
    #hypodepth : 7
    #width : 8
    #rjb: 9
    #rx: 10
    #depthtotop: 11
    #ry : 12
    
#for i in range(ztest.shape[0]):
    
        # Distance
        
dx = base.DistancesContext()
dx.rjb=    np.array([ztest[i,9]])



#dx.rjb = np.logspace(-1, 2, 10)
# Magnitude and rake
rx = base.RuptureContext()
rx.mag = np.array([ztest[i,0]])
rx.rake = np.array([ztest[i,5]])
rx.hypo_depth = np.array([ztest[i,7]])
# Vs30
sx = base.SitesContext()
sx.vs30 = np.array([ztest[i,2]])
sx.vs30measured = 0

dx.rrup=np.array([ztest[i,1]])
rx.ztor=np.array([ztest[i,11]])
rx.dip=np.array([ztest[i,6]])
rx.width=np.array([ztest[i,8]])
dx.rx=np.array([rxkeep[i]])
dx.ry0=np.array([0])
sx.z1pt0= np.array([ztest[i,3]])
sx.z2pt5=np.array([ztest[i,4]])

# Evaluate GMPE
#Unit of measure for Z1.0 is [m] (ASK)
#lmean, lsd = gmpeASK.get_mean_and_stddevs(sx, rx, dx, imt.PGV(), stddev_types)
i=0
#for period1 in period:
for ii in range(0,6):
    sx.vs30measured = 0
    period1=period[ii]
    gmpeBSSAdata[ii], g = gmpeBSSA.get_mean_and_stddevs(sx, rx, dx, imt.SA(period1), stddev_types)
    gmpeBSSAstd[ii]=g[0][0]
    
    gmpeCBdata[ii], g = gmpeCB.get_mean_and_stddevs(sx, rx, dx, imt.SA(period1), stddev_types)
    gmpeCBstd[ii]=g[0][0]
    
    gmpeCYdata[ii], g = gmpeCY.get_mean_and_stddevs(sx, rx, dx, imt.SA(period1), stddev_types)
    gmpeCYstd[ii]=g[0][0]
    
    sx.vs30measured = [0]
    gmpeASKdata[ii], g = gmpeASK.get_mean_and_stddevs(sx, rx, dx, imt.SA(period1), stddev_types)
    gmpeASKstd[ii]=g[0][0]
        









#m=pygmm.BooreStewartSeyhanAtkinson2014(mag=6.0, dist_jb=76, dist_x=50, dist_rup=25, dip=90, v_s30=400)
#boore=m.interp_ln_stds(periods=period)
boore=gmpeBSSAstd
plt.semilogx(period,boore,label='BSSA')

#m=pygmm.AbrahamsonSilvaKamai2014(mag=6.0, dist_jb=76, dist_x=50, dist_rup=25, dip=90, v_s30=400, mechanism='SS')
#boore=m.interp_ln_stds(periods=period)
boore=gmpeASKstd
plt.semilogx(period,boore,label='ASK')

#m=pygmm.CampbellBozorgnia2014(mag=6.0, dist_jb=76, dist_x=50, dist_rup=25, dip=90, v_s30=400, mechanism='SS')
#boore=m.interp_ln_stds(periods=period)
boore=gmpeCBstd
plt.semilogx(period,boore,label='CB')

#m=pygmm.ChiouYoungs2014(mag=6.0, dist_jb=76, dist_x=50, dist_rup=25, dip=90, v_s30=400)
#boore=m.interp_ln_stds(periods=period)
boore=gmpeCYstd
plt.semilogx(period,boore,label='CY')


plt.legend()
plt.grid()

with open('diffg.txt', 'wb') as filehandle:  
    # store the data as binary data stream
    pickle.dump(boore, filehandle)


#original_std=train_targets.std(axis=0)

#train_targets

#plot std/mean/residusals
#diff=np.std(train_targets-b,axis=0)
#diffmean=np.mean(train_targets-b,axis=0)
f22=plt.figure('Diff of Residuals vs period3 ')
ggg=train_targets-b
for i in range(6):
    plt.semilogx(period[i]*np.ones(train_data.shape[0]),ggg[:,i],'.b',label='Residuals',alpha=.2)   
    plt.plot([period[i],period[i]],[-diff[i],diff[i]],color='r')
plt.semilogx(period,diffmean,linestyle='--', marker='o', color='g',label='Mean')
#plt.errorbar(period, np.zeros(period.size), yerr=diff,color='r',label='Std',fmt='.')
#plt.semilogx(period,diff,label='Std')
plt.xlabel('Period')
plt.ylabel('Residuals')
plt.title('Residuals, Standard Deviation, and Mean vs Period')
#plt.legend()
plt.grid()
plt.show
#plt.errorbar(period, np.zeros(period.size),xerr=np.zeros(period.size), yerr=diff,color='r',label='Std')

#plt.semilogx.errorbar(x, y + 3, xerr=0.0, yerr=yerr, label='both limits (default)')


f22=plt.figure('Diff of Residuals vs period2 ')
ggg=train_targets-b
#for i in range(85):
#    plt.semilogx(period[i]*np.ones(2400),ggg[:,i],'.b',label='Residuals',alpha=.2)   
plt.semilogx(period,diffmean,linestyle='--', marker='o', color='g',label='Mean')
#plt.semilogx.errorbar(period, np.zeros(period.size), yerr=diff,color='r',label='Std',fmt='.')
#plt.semilogx(period,diff,label='Std')
plt.xlabel('Period')
plt.ylabel('Residuals')
plt.title('Residuals')
plt.legend()
plt.grid()

#origvalues=(train_data/2.*(keep1-keep2))+keep3
##orig1=train_data*keepmax
##origvalues=aa.inverse_transform(orig1)



f22=plt.figure('Residuals vs Mw at T = 1 s')
ggg=train_targets-b
plt.plot(origvalues[:,0],ggg[:,2],'.b',label='Residuals',alpha=.4)
plt.xlabel('Mw')
plt.ylabel('Residuals')
plt.title('Residuals vs Mw at T = 5 s')
plt.legend()
plt.grid()

f22=plt.figure('Residuals vs R_JB at T = 5 s')
ggg=train_targets-b
plt.plot((origvalues[:,1]),ggg[:,2],'.b',label='Residuals',alpha=.4)
plt.xlabel('R_JB')
plt.ylabel('Residuals')
plt.title('Residuals R_rup at T = 1 s')
plt.legend()
plt.grid()

f22=plt.figure('Residuals vs Vs30 at T = 5 s')
ggg=train_targets-b
plt.plot((origvalues[:,2]),ggg[:,2],'.b',label='Residuals',alpha=.4)
plt.xlabel('Vs30')
plt.ylabel('Residuals')
plt.title('Residuals Vs30 at T = 5 s')
plt.legend()
plt.grid()


def garson(A, B):
    """
    Computes Garson's algorithm
    A = matrix of weights of input-hidden layer (rows=input & cols=hidden)
    B = vector of weights of hidden-output layer
    """
    B = np.diag(B)

    # connection weight through the different hidden node
    cw = np.dot(A, B)

    # weight through node (axis=0 is column; sum per input feature)
    cw_h = abs(cw).sum(axis=0)

    # relative contribution of input neuron to outgoing signal of each hidden neuron
    # sum to find relative contribution of input neuron
    rc = np.divide(abs(cw), abs(cw_h))
    rc = rc.sum(axis=1)

    # normalize to 100% for relative importance
    ri = rc / rc.sum()
    return(ri)
    
def connection_weights(A, B):
    """
    Computes Connection weights algorithm
    A = matrix of weights of input-hidden layer (rows=input & cols=hidden)
    B = matrix of weights of hidden-output layer (rows=hidden & cols=output)
    """    
    cw = np.abs(np.dot(A, B))

    # normalize to 100% for relative importance
    ri = cw / cw.sum()
    return(ri)    
    
weights=model.get_weights()
weights1=weights[0]
weights2=weights[1]
weights3=weights[2]

depend=[]
for i in range(6):
#    depend.append(garson(weights1,weights3[:,i]))
    depend.append(garson(weights1,weights2))
depend=np.array(depend)

#depend=np.abs(connection_weights(weights1, weights3))
#depend=(connection_weights(weights1, weights3))
#plt.close('all')
cw = np.abs(np.dot(weights1, weights3))
summ1=np.sum(cw,axis=0)
ri = cw/summ1
depend=ri
#,figsize=(7,7)
f22=plt.figure('Significance of parame3t3ers43343')
for i in range(0,train_data.shape[1]):
    plt.semilogx(period,100*depend[i,:],label=feature_names[i])
#plt.semilogx(period[0:58],depend[1,:],label='R_JB')
#plt.semilogx(period[0:58],depend[2,:],label='Vs30')
#plt.semilogx(period[0:58],depend[3,:],label='Z1.0')
#plt.semilogx(period[0:58],depend[4,:],label='Z2.5')
#plt.semilogx(period[0:58],depend[5,:],label='Z_tor')

#plt.semilogx(period[0:58],depend[6,:],label='b1')
#plt.semilogx(period[0:58],depend[7,:],label='b2')
#plt.semilogx(period[0:58],depend[8,:],label='b3')
#plt.semilogx(period[0:58],depend[9,:],label='b4')
#plt.semilogx(period[0:58],depend[10,:],label='b5')
#plt.semilogx(period[0:58],depend[11,:],label='b6')
#plt.semilogx(period,depend[6,:],label='b')
#plt.semilogx(period,depend[7,:],label='c')
#plt.semilogx(period,depend[8,:],label='d')
#plt.semilogx(period,depend[9,:],label='e')
plt.xlabel('Period')
plt.ylabel('Significance (%)')
plt.title('Significance of Parameters (%)')
#plt.legend()
plt.grid()
plt.xlim([0.1,10])

#sepctral acdleation at 1 sec versus period
predictgmpe=np.zeros(300)
for i in np.arange(0,300):
    m=pygmm.BooreStewartSeyhanAtkinson2014(mag=6, dist_jb=i, dip=90, v_s30=393,depth_1_0 =0.329,depth_2_5 =1.642,depth_tor=6.8)
    m.interp_ln_stds(periods=period)
    predictgmpe[i]=m.spec_accels[67]
 #m.periods[67]
#ztest = np.column_stack([Mwtest,distest,vs30test, z1test, z2p5test,ztt,aziumuthtest])
gmpe_data=np.zeros([300,6])


#array([    6.35102006,   120.5014957 ,   393.94584814,    88.67761032,
#         329.90647564,  1425.55618911])
gmpe_data[:,0]=6.0
gmpe_data[:,1]=np.arange(0,300)
gmpe_data[:,2]=393.
#gmpe_data[:,3]=90
gmpe_data[:,3]=329
gmpe_data[:,4]=1642
gmpe_data[:,5]=6.8
#gmpe_data[:,6]=0



#gmpe_data[0,1]=0.3
#gmpe_data[1,1]=0.5
#gmpe_data_n=aa.transform(gmpe_data)
#gmpe_data_n=gmpe_data_n/keepmax
#
#a=model.predict(gmpe_data_n)
##b=model.predict(train_data)
#f1fa=plt.figure('GMPE vs. Model Prediction, T = 1.0 S')
#plt.plot((gmpe_data[:,1]),np.exp(a[:,41]),'r',label='model_prediction')
#plt.loglog((gmpe_data[:,1]),predictgmpe,'b',label='GMPE')
#plt.xlabel('distance (km)')
#plt.ylabel('SA')
#plt.title('SA, T = 1.0 S')
#plt.grid()
#plt.legend()
#plt.show()



#test_data = np.column_stack([Mwtest,distest,vs30test,z10test,z25test,raketest,diptest,striketest, hypodepthtest, lattest, longtest,hypolattest,hypolontest,widthtest,
#                             lengthtest,rjbtest,rxtest,rytest,hypodistest,startdepthtest])
fig=plt.figure('Diff of Residuals vs periodnow2, lat long ')
#fig, ax = plt.subplots(2)
from matplotlib import cm
import matplotlib as mpl
#cmap = plt.get_cmap('bwr')
cmap = plt.get_cmap('coolwarm')

#fig.subplots_adjust(top=0.9)
#plt.plot([0,1],[0,1],'-',color=cmap(1))
ggg=train_targets-b
tempg=ggg[:,2]
normalize = mpl.colors.Normalize(vmin=np.min(tempg), vmax=np.max(tempg))
#for i in range(test_data.shape[0]):

test_data1.shape

index=(test_data1[:,0]<7)
test_data2=test_data1[index]

import random
tempint=random.sample(range(1, test_data1.shape[0]), 2000)

for i in tempint:
        #plt.plot([ztest[i,9],ztest[i,10]],[ztest[i,11],ztest[i,12]],'-',color=cmap(1))
        plt.plot([test_data2[i,10],test_data2[i,12]],[test_data2[i,9],test_data2[i,11]],'-',color=cmap(normalize(tempg[i])))
        plt.plot(test_data2[i,12],test_data2[i,11], marker='*',markerfacecolor='k',markeredgecolor='k',markersize=12)

#plt.plot(df.Hypocenter_Lon[row],df.Hypocenter_Lat[row], marker='*',markerfacecolor='k',markersize=12)
#[ztest1[i,9],ztest1[i,11]]

#train_data = np.column_stack([Mwtrain,distrain,vs30train,z10train,z25train,raketrain,diptrain,striketrain,hypodepthtrain, lattrain, longtrain, hypolattrain,hypolontrain,widthtrain,
#lengthtrain,rjbtrain,rxtrain,rytrain,hypodistrain,startdepthtrain])#, Utrain, Ttrain, xitrain])


fig, ax = plt.subplots(figsize=(6, 1))
fig.subplots_adjust(bottom=0.5)

norm = mpl.colors.Normalize(vmin=np.min(tempg), vmax=np.max(tempg))

cb1 = mpl.colorbar.ColorbarBase(ax, cmap=cmap,
                                norm=norm,
                                orientation='horizontal')
cb1.set_label('Some Units')
fig.show()

plt.show()




#The fault surface can be fully described by the upper trace, dip, dip direction, and upper/lower depths.